{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for processing the data that I have, aligning labels for decoding\n",
    "Should import with py-scipystack\n",
    "For working with Sherlock's interface, need to import modules. Need to have specific module loading for it to work otherwise go to the input.sh and run those commands in this notebook. \n",
    "(here is the sequence that works: py-scipystack math py-autograd py-pytorch cuda praat )\n",
    "\n",
    "What features do we want?\n",
    "\n",
    "We have:\n",
    "1. phonemes per time\n",
    "2. silence\n",
    "3. F0 pitch per time (individually on Praat)\n",
    "4. dB per time (individually audio on Praat with line separation)\n",
    "\n",
    "TODO: (see more planning doc for more scripts)\n",
    "1. Volume (db); this is described as \"intensity\" on praat\n",
    "2. pitch characteristics (spectrogram?) (can get pitch Hz from the github script on praat: https://github.com/lennes/pitch-distributions/blob/master/collectPitchSamplesFromCorpus.praat)\n",
    "   1. idea here: may be able to use strongest power frequencies or extract from Praat (corroborate from Praat)\n",
    "   2. corroborate with MFA phonemes that are vocal\n",
    "\n",
    "Decoder characteristics:\n",
    "1. silence vs sound\n",
    "2. speech envelope (volume)\n",
    "3. maybe discretize pitch first into a few categories\n",
    "   -for pitch, probably need to normalize? and do relative pitch from average or something\n",
    "\n",
    "Create jupyter instance\n",
    "check dependencies\n",
    "figure out how to export Praat alignments and textgrids\n",
    "parse those into bins\n",
    "align audios with the cues that we have\n",
    "align labels with cues that we have\n",
    "\n",
    "other question: do we need to align at all? can bypass and use bin time of 20ms to align. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task audio, mic audio\n",
    "\n",
    "Use 'trial_start_nsp_analog_time', 'trial_end_nsp_analog_time'\n",
    "To align with nap time\n",
    "\n",
    "Check with last trial end in ns5\n",
    "\n",
    "Audio at 30KHz\n",
    "\n",
    "Can play audio back from ns5\n",
    "\n",
    "20ms binsize\n",
    "\n",
    "maybe also get the neural PSTHs around word onset?\n",
    "\n",
    "#delay time start\n",
    "#check if there is a delay with the ns5 data (use blackrock documentation)\n",
    "#load ns5 data--> one isneural, another is analog, other is digital\n",
    "#this has audio stream. check to make sure audio stream is aligned with what we expect with the 20ms bin timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testin2g2\n"
     ]
    }
   ],
   "source": [
    "#assume that labels are ready in 20ms time intervals\n",
    "#assume we know alignment thru redis clock#tet###fasdffasd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "ts_filepath = '/home/groups/henderj/rzwang/exp_data/prosody/t12-02-20-2025/20250220_082208_(1).mat'\n",
    "# 5. Load redis MATLAB file with nsp timestamp information.\n",
    "mat_data = sio.loadmat(ts_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['block_number', 'block_start_time', 'all_candidate_sentence_acoustic_scores', 'graph_name', 'confirmation_epoch_start_redis_time', 'trial_start_nsp_analog_time', 'trial_end_nsp_analog_time', 'delay_duration_ms', 'all_candidate_sentence_oldlm_scores', 'cue3_start_nsp_analog_time', 'decoder_signal', 'all_candidate_sentence_newlm_scores', 'binned_neural_spike_band_power', 'session_description', 'confirmation_epoch_start_nsp_analog_time', 'binned_neural_redis_clock', 'binned_neural_threshold_crossings', 'task_type', 'participant', 'trial_start_redis_time', 'corrected_with_candidate', 'microphone_nsp_time', 'correct_status', 'inter_trial_duration_ms', 'trial_end_nsp_neural_time', 'cue', 'all_candidate_sentences', 'block_description', 'ngram_decoder_partial_output', '__version__', 'word_count_mismatch', 'norm_channel_stds', 'trial_accuracy_confirmation', 'binned_neural_nsp_timestamp', 'cue3', 'cue2_start_nsp_neural_time', 'trial_end_redis_time', 'cue3_start_redis_time', 'cue2_start_redis_time', 'trial_paused_by_CNRA', 'cue3_start_nsp_neural_time', 'confirmation_epoch_start_nsp_neural_time', 'go_cue_nsp_neural_time', 'final_decoded_sentence', 'button_pressed', 'all_candidate_sentence_total_scores', 'decoder_logit_output', 'cue2', 'cue1', 'using_correct_electrode_mapping', 'microphone_data', 'go_cue_redis_time', '__header__', '__globals__', 'norm_redis_times', 'trial_start_nsp_neural_time', 'go_cue_nsp_analog_time', 'current_decoding_context_string', 'norm_channel_means', 'cue2_start_nsp_analog_time', 'session_name', 'ngram_decoder_final_output', 'decoder_output_redis_clock', 'trial_timed_out']\n"
     ]
    }
   ],
   "source": [
    "print(mat_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1740068451564 1740068451584 1740068451604 ... 1740068502438\n",
      "  1740068502458 1740068502478]]\n",
      "(1, 2547)\n",
      "[[1740068439]]\n",
      "(2547, 256)\n",
      "(2547, 256)\n",
      "[]\n",
      "[u\"scaled_100wpm_POD0000000240_S0000228.wav;AND IF IT'S MICHAEL;4;1.650000000000091;podcast;2.42;1.82;1.45                  \"\n",
      " u'scaled_100wpm_POD0000000320_S0000205.wav;I CAN PLAY THE MOVIE;5;1.5399999999999636;podcast;3.25;2.44;1.95                '\n",
      " u\"scaled_100wpm_POD0000000416_S0000055.wav;WHO ARE COGNIZANT OF WHAT'S HAPPENING;6;2.7299999999999613;podcast;2.2;1.65;1.32\"\n",
      " u'scaled_100wpm_AUD0000001043_S0000823.wav;NONE FOR ME;3;1.2300000000000182;audiobook;2.44;1.83;1.46                       '\n",
      " u'scaled_100wpm_POD0000000352_S0000314.wav;MONDAY BEGAN LIKE A NORMAL DAY;6;1.8199999999999363;podcast;3.3;2.47;1.98       ']\n",
      "[[1.72667492e+18 1.72667492e+18 1.72667492e+18 ... 1.72667497e+18\n",
      "  1.72667497e+18 1.72667497e+18]]\n",
      "[[1.72667492e+18 1.72667494e+18 1.72667495e+18 1.72667495e+18\n",
      "  1.72667496e+18]]\n",
      "[[1.74006846e+12 1.74006847e+12 1.74006848e+12 1.74006849e+12\n",
      "  1.74006850e+12]]\n",
      "[[1.74006845e+12 1.74006847e+12 1.74006847e+12 1.74006848e+12\n",
      "  1.74006849e+12]]\n",
      "[[1.74006847e+12 1.74006847e+12 1.74006848e+12 1.74006849e+12\n",
      "  1.74006850e+12]]\n"
     ]
    }
   ],
   "source": [
    "#mat_data probing. Let's go on redis clock, rather than nsp time (since bins are currently in redis time)\n",
    "#bins should be 20ms bins\n",
    "print(mat_data['binned_neural_redis_clock']) #starts at 1740068451564, ends 1740068502478. Probably the end of bins? or the start of bins\n",
    "#todo: check whether bin times are start or end boundaries\n",
    "print(mat_data['binned_neural_redis_clock'].shape) #1x2547 long\n",
    "print(mat_data['block_start_time']) #1740068439\n",
    "print(mat_data['binned_neural_spike_band_power'].shape) #array of floats, 2547x256 (time x array)\n",
    "print(mat_data['binned_neural_threshold_crossings'].shape) #binary array, 2547x256 (time x array)\n",
    "print(mat_data['task_type'])\n",
    "print(mat_data['cue']) #array of cue lines (audio file, corresponding transcript, number of words. can use this to match trials with labels/audio files, potentially)\n",
    "#side note: I also have block .txt files that have this information\n",
    "print(mat_data['binned_neural_nsp_timestamp'])\n",
    "print(mat_data['go_cue_nsp_neural_time'])\n",
    "print(mat_data['go_cue_redis_time']) #go cues for each trial, on the redis clock\n",
    "print(mat_data['trial_start_redis_time']) #trial start times on the redis clock\n",
    "print(mat_data['trial_end_redis_time']) #trial end times on the redis clock\n",
    "\n",
    "\n",
    "#I think it's the binned_neural_redis_clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block_start = mat_data['block_start_time'][0][0]  # 1740068439\n",
    "\n",
    "bin_times = mat_data['binned_neural_redis_clock'][0]  # 1x2547 array\n",
    "\n",
    "# Bins appear to be 20ms (difference between consecutive times is 20)\n",
    "# Trial-specific timing\n",
    "\n",
    "trial_starts = mat_data['trial_start_redis_time'][0]\n",
    "trial_ends = mat_data['trial_end_redis_time'][0]\n",
    "go_cues = mat_data['go_cue_redis_time'][0]\n",
    "cues = mat_data['cue']  # Contains audio filename, transcript, word count, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mat_to_npy(mat_file_path, output_dir):\n",
    "    mat_data = sio.loadmat(mat_file_path)\n",
    "    np.save(f\"{output_dir}/spike_band_power.npy\", mat_data['binned_neural_spike_band_power'])\n",
    "    np.save(f\"{output_dir}/threshold_crossings.npy\",mat_data['binned_neural_threshold_crossings'])\n",
    "    np.save(f\"{output_dir}/bin_times.npy\", mat_data['binned_neural_redis_clock'][0])\n",
    "    np.save(f\"{output_dir}/go_times.npy\", mat_data['go_cue_redis_time'][0])\n",
    "    np.save(f\"{output_dir}/trial_starts.npy\", mat_data['trial_start_redis_time'][0])\n",
    "    np.save(f\"{output_dir}/trial_ends.npy\", mat_data['trial_start_redis_time'][0])\n",
    "    np.save(f\"{output_dir}/cues.npy\", mat_data['cue'][])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "#TODO: add the theshold crossings, as this code only has the neural spike band power\n",
    "def process_all_blocks(exp_dir, label_dir, output_dir, footer):\n",
    "    \"\"\"\n",
    "    Process all blocks in directory and save aligned data as .npy files. Should overwrite existing label files in the folder (if any)\n",
    "    Parameters:\n",
    "    -----------\n",
    "    exp_dir : str\n",
    "        Directory containing .mat files with the neural data\n",
    "    label_dir : str\n",
    "        Directory containing label files (should be .npy). should correspond to the 20ms bins of the neural data\n",
    "    output_dir : str\n",
    "        Directory to save processed .npy files\n",
    "    footer : str\n",
    "        file label footer (vs .wav file)\n",
    "        for example, if a.wav has labels a_lbl.npy, this string would be \"_lbl.npy\"\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    all_neural_data = []\n",
    "    all_neural_data_tc = []\n",
    "    all_labels = []\n",
    "    trial_info = []  # Store metadata about each trial\n",
    "    \n",
    "    #Process each .mat file in directory\n",
    "    mat_files = [f for f in os.listdir(exp_dir) if f.endswith('.mat')]\n",
    "    for i, mat_file in enumerate(mat_files):\n",
    "#         print(f\"Processing block {i+1}/{len(mat_files)}: {mat_file}\")\n",
    "        print(\"Processing block {}/{}: {}\".format(i + 1, len(mat_files), mat_file))\n",
    "        # Load mat file\n",
    "        mat_data = sio.loadmat(os.path.join(exp_dir, mat_file))\n",
    "        \n",
    "        # Get block data\n",
    "        neural_data = mat_data['binned_neural_spike_band_power'] #we should also\n",
    "        #include the binned_neural_threshold_crossings here?\n",
    "        neural_data_tc = mat_data['binned_neural_threshold_crossings']\n",
    "        bin_times = mat_data['binned_neural_redis_clock'][0]\n",
    "        go_cues = mat_data['go_cue_redis_time'][0]\n",
    "        trial_ends = mat_data['trial_end_redis_time'][0]\n",
    "        \n",
    "        #Process each trial in block\n",
    "        for trial_idx, (cue_line, go_cue, trial_end) in enumerate(zip(mat_data['cue'], go_cues, trial_ends)):\n",
    "            #Parse audio filename\n",
    "            audio_file = cue_line.split(';')[0]\n",
    "            \n",
    "            # Load corresponding labels\n",
    "            label_file = os.path.join(label_dir, audio_file.replace('.wav', footer)) #TODO: change this if doing a different label type\n",
    "            if not os.path.exists(label_file):\n",
    "#                 print(f\"Warning: Missing label file for {audio_file}\")\n",
    "                print(\"Warning: Missing label file for {}\".format(audio_file))\n",
    "                continue\n",
    "                \n",
    "            labels = np.load(label_file)\n",
    "            \n",
    "            # Find trial boundaries\n",
    "            trial_start_idx = np.searchsorted(bin_times, go_cue)\n",
    "            trial_end_idx = np.searchsorted(bin_times, trial_end)\n",
    "            \n",
    "            # Extract trial data\n",
    "            trial_neural = neural_data[trial_start_idx:trial_end_idx]\n",
    "            trial_neural_tc = neural_data_tc[trial_start_idx:trial_end_idx]\n",
    "            \n",
    "            # Align lengths\n",
    "            n_neural_bins = trial_neural.shape[0] #should be the same for tc and sbp\n",
    "            n_label_bins = len(labels)\n",
    "            \n",
    "            if n_neural_bins > n_label_bins:\n",
    "                labels = np.pad(labels, (0, n_neural_bins - n_label_bins),\n",
    "                              mode='constant', constant_values=1)\n",
    "            else:\n",
    "                labels = labels[:n_neural_bins]\n",
    "                trial_neural = trial_neural[:n_label_bins]\n",
    "                trial_neural_tc = trial_neural_tc[:n_label_bins]\n",
    "            \n",
    "            # Store aligned data\n",
    "            all_neural_data.append(trial_neural)\n",
    "            all_neural_data_tc.append(trial_neural_tc)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            # Store trial metadata\n",
    "            trial_info.append({\n",
    "                'block_file': mat_file,\n",
    "                'audio_file': audio_file,\n",
    "                'trial_idx': trial_idx,\n",
    "                'n_bins': len(labels)\n",
    "            })\n",
    "    \n",
    "    # Convert to arrays and save\n",
    "    neural_data_array = np.concatenate(all_neural_data, axis=0)\n",
    "    neural_data_array_tc = np.concatenate(all_neural_data_tc, axis=0)\n",
    "    labels_array = np.concatenate(all_labels)\n",
    "    \n",
    "#     np.save(f\"{output_dir}/neural_data.npy\", neural_data_array)\n",
    "#     np.save(f\"{output_dir}/labels.npy\", labels_array)\n",
    "#     np.save(f\"{output_dir}/trial_info.npy\", trial_info)\n",
    "    \n",
    "#     print(f\"Processed {len(mat_files)} blocks, {len(trial_info)} trials\")\n",
    "#     print(f\"Total timepoints: {len(labels_array)}\")\n",
    "    np.save(\"{}/neural_data_sbp.npy\".format(output_dir), neural_data_array) #spike band power \n",
    "    np.save(\"{}/neural_data_tc.npy\".format(output_dir), neural_data_array_tc) #spike band power \n",
    "    np.save(\"{}/labels.npy\".format(output_dir), labels_array)\n",
    "    np.save(\"{}/trial_info.npy\".format(output_dir), trial_info)\n",
    "    \n",
    "    print(\"Processed {} blocks, {} trials\".format(len(mat_files), len(trial_info)))\n",
    "    print(\"Total timepoints: {}\".format(len(labels_array)))\n",
    "    return neural_data_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.13 (default, Apr 27 2017, 14:19:21) \n",
      "[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)]\n",
      "\u001b[1;31mLmod Warning: \u001b[0m\n",
      "\u001b[1;31m-------------------------------------------------------------------------------\u001b[0m\n",
      "The following dependent module(s) are not currently loaded: cudnn/7.0.5\n",
      "(required by: py-pytorch/0.3.0_py27), python/2.7.13 (required by:\n",
      "py-scipystack/1.0_py27, py-pytorch/0.3.0_py27)\n",
      "\u001b[1;31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) libressl/2.5.3 => libressl/3.2.1     3) sqlite/3.18.0 => sqlite/3.44.2\n",
      "  2) python/2.7.13 => python/3.9.0\n",
      "\n",
      "\u001b[1;31mLmod Warning: \u001b[0m\n",
      "\u001b[1;31m-------------------------------------------------------------------------------\u001b[0m\n",
      "The following dependent module(s) are not currently loaded: cudnn/6.0 (required\n",
      "by: torch/20180202), python/3.9.0 (required by: py-jupyter/1.0.0_py39,\n",
      "py-numpy/1.20.3_py39)\n",
      "\u001b[1;31m-------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The following have been reloaded with a version change:\n",
      "  1) cuda/8.0.61 => cuda/9.1.85     2) cudnn/6.0 => cudnn/7.0.5\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing block 1/14: 20250220_092958_(12).mat\n",
      "Processing block 2/14: 20250220_094931_(15).mat\n",
      "Processing block 3/14: 20250220_084811_(5).mat\n",
      "Processing block 4/14: 20250220_091803_(10).mat\n",
      "Processing block 5/14: 20250220_084201_(4).mat\n",
      "Processing block 6/14: 20250220_083525_(3).mat\n",
      "Processing block 7/14: 20250220_085412_(6).mat\n",
      "Processing block 8/14: 20250220_082717_(2).mat\n",
      "Processing block 9/14: 20250220_093742_(13).mat\n",
      "Processing block 10/14: 20250220_091212_(9).mat\n",
      "Processing block 11/14: 20250220_090001_(7).mat\n",
      "Processing block 12/14: 20250220_092404_(11).mat\n",
      "Processing block 13/14: 20250220_094329_(14).mat\n",
      "Processing block 14/14: 20250220_090631_(8).mat\n",
      "Processed 14 blocks, 280 trials\n",
      "Total timepoints: 46171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[151.12434 , 116.256424,  80.161865, ...,  99.960754,  49.764416,\n",
       "         55.563038],\n",
       "       [115.588844,  90.225296, 101.437965, ...,  82.24857 ,  50.258266,\n",
       "         52.168068],\n",
       "       [ 87.49782 ,  87.25968 , 101.88918 , ..., 179.96828 ,  49.59959 ,\n",
       "         54.988564],\n",
       "       ...,\n",
       "       [105.395996,  92.8964  ,  94.88707 , ...,  86.77444 ,  40.54669 ,\n",
       "         63.69227 ],\n",
       "       [105.838135,  94.94759 ,  96.397606, ..., 100.22508 ,  46.997364,\n",
       "         54.956276],\n",
       "       [105.87013 ,  87.94528 , 115.41392 , ..., 111.47727 ,  47.287506,\n",
       "         54.489246]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dir = \"/home/groups/henderj/rzwang/labels\"\n",
    "exp_dir = \"/home/groups/henderj/rzwang/blocks/t12-02-20-2025\"\n",
    "output_dir = \"/home/groups/henderj/rzwang/processed_data\"\n",
    "\n",
    "process_all_blocks(exp_dir, label_dir, output_dir, \"_silence.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder below didn't work, so moved the model to a .py and ran from there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 0 is not a Variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-03fd18d1d569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                      \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                      learning_rate=0.001)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# Save the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-03fd18d1d569>\u001b[0m in \u001b[0;36mtrain_decoder\u001b[0;34m(neural_data, labels, window_size, stride, batch_size, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/software/user/open/py-pytorch/0.3.0_py27/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-03fd18d1d569>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/software/user/open/py-pytorch/0.3.0_py27/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/software/user/open/py-pytorch/0.3.0_py27/lib/python2.7/site-packages/torch/nn/modules/container.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/software/user/open/py-pytorch/0.3.0_py27/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/software/user/open/py-pytorch/0.3.0_py27/lib/python2.7/site-packages/torch/nn/modules/conv.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 166\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/software/user/open/py-pytorch/0.3.0_py27/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0m_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 0 is not a Variable"
     ]
    }
   ],
   "source": [
    "#pilot of the model\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "# from torchsummary import summary\n",
    "from scipy import stats\n",
    "\n",
    "# Function to create windowed data\n",
    "def create_windows(data, window_size, stride=1):\n",
    "    \"\"\"\n",
    "    Create windowed data for both neural data and labels\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        Neural data (n_timepoints, n_channels)\n",
    "    window_size : int\n",
    "        Number of time bins in each window\n",
    "    stride : int\n",
    "        Number of bins to stride between windows\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    windowed_data : np.ndarray\n",
    "        Windowed data (n_windows, n_channels, window_size)\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = data.shape[0]\n",
    "    n_windows = ((n_samples - window_size) // stride) + 1\n",
    "\n",
    "    windows = np.zeros((n_windows, data.shape[1], window_size))\n",
    "\n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        # Transpose to get (n_channels, window_size)\n",
    "        windows[i] = data[start_idx:end_idx].T\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "# Training script modifications\n",
    "def train_decoder(neural_data, labels, window_size=5, stride=1):\n",
    "    # Create windowed data\n",
    "    X = create_windows(neural_data, window_size, stride)\n",
    "\n",
    "    # For labels, take the mode within each window\n",
    "    y = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        y[i] = np.mean(labels[start_idx:end_idx])\n",
    "\n",
    "    #Convert to PyTorch tensors\n",
    "    X = torch.FloatTensor(X)\n",
    "    y = torch.FloatTensor(y)\n",
    "\n",
    "    #Create model and train\n",
    "    model = CNNSilenceDecoder(n_channels=neural_data.shape[1],\n",
    "                             window_size=window_size)\n",
    "\n",
    "    #now train the decoder\n",
    "\n",
    "\n",
    "\n",
    "class CNNSilenceDecoder(nn.Module):\n",
    "    def __init__(self, n_channels=256, window_size=5):\n",
    "        super(CNNSilenceDecoder, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32 * window_size, 1),\n",
    "            nn.Sigmoid() #output sigmoid for binary guess\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_decoder(neural_data, labels, window_size=5, stride=1, batch_size=32, n_epochs=10, learning_rate=0.001):\n",
    "    # Create windowed data\n",
    "    X = create_windows(neural_data, window_size, stride)\n",
    "    \n",
    "    #For labels, take the mode within each window\n",
    "    y = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        y[i] = round(np.mean(labels[start_idx:end_idx])) #round should give mode for binary\n",
    "    \n",
    "    # Split into train/test sets (80/20 split)\n",
    "    n_samples = len(X)\n",
    "    n_train = int(0.8 * n_samples)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    train_indices = indices[:n_train]\n",
    "    test_indices = indices[n_train:]\n",
    "    \n",
    "    X_train = torch.FloatTensor(X[train_indices])\n",
    "    y_train = torch.FloatTensor(y[train_indices])\n",
    "    X_test = torch.FloatTensor(X[test_indices])\n",
    "    y_test = torch.FloatTensor(y[test_indices])\n",
    "    \n",
    "    #Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    #Initialize model and optimizer\n",
    "    model = CNNSilenceDecoder(n_channels=neural_data.shape[1], window_size=window_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #TODO: do we want to use Adam\n",
    "    criterion = nn.BCELoss() #TODO: change this loss function from binary cross entrop\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test.unsqueeze(1))\n",
    "            test_preds = (test_outputs > 0.5).float()\n",
    "            accuracy = (test_preds == y_test.unsqueeze(1)).float().mean()\n",
    "        \n",
    "        print(\"Epoch {}/{}: Train Loss = {:.4f}, Test Loss = {:.4f}, Test Accuracy = {:.4f}\".format(\n",
    "            epoch+1, n_epochs, total_loss/len(train_loader), test_loss.item(), accuracy.item()))\n",
    "    \n",
    "    return model\n",
    "\n",
    "neural_data = np.load(\"/home/groups/henderj/rzwang/processed_data/neural_data.npy\")\n",
    "labels = np.load(\"/home/groups/henderj/rzwang/processed_data/labels.npy\")\n",
    "\n",
    "# Train the model\n",
    "window_size = 5  # 100ms window (assuming 20ms bins)\n",
    "stride = 1\n",
    "model = train_decoder(neural_data, labels, \n",
    "                     window_size=window_size, \n",
    "                     stride=stride,\n",
    "                     batch_size=16,\n",
    "                     n_epochs=3,\n",
    "                     learning_rate=0.001)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"/home/groups/henderj/rzwang/decoders/silence_decoder.pt\")\n",
    "\n",
    "def evaluate_model(model, neural_data, labels, window_size=5, stride=1):\n",
    "    model.eval()\n",
    "    X = create_windows(neural_data, window_size, stride)\n",
    "    y = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        start_idx = i * stride\n",
    "        end_idx = start_idx + window_size\n",
    "        y[i] = np.mean(labels[start_idx:end_idx])\n",
    "    \n",
    "    X = torch.FloatTensor(X)\n",
    "    y = torch.FloatTensor(y)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        accuracy = (preds.squeeze() == y).float().mean()\n",
    "    \n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
